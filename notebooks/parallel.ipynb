{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596699824160",
   "display_name": "Python 3.8.3 64-bit ('.env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to GIL (Global Interpreter Lock) each python interpreter can only run on a single processor. So for parallel computations we need to lauch multiple interpreters under the hood.\n",
    "\n",
    "Concurrency vs Parallelism:\n",
    "* **Concurrency**: In a uniprocessor machine, multiple processes created through multiprocessing package are context switched by the OS, which results in concurrency.\n",
    "* **Concurrency and Parallelism**: In a multiprocessor machine, multiple processes created through this package are executed in multiple processors along with context switching, providing both concurrency and parallelism.\n",
    "\n",
    "\n",
    "### ways to start a process\n",
    "\n",
    "* **spawn** : \n",
    "The parent process starts a fresh python interpreter process. The child process will only inherit those resources necessary to run the process objects run() method. In particular, unnecessary file descriptors and handles from the parent process will not be inherited. Starting a process using this method is rather slow compared to using fork or forkserver.\n",
    "Available on Unix and Windows. The default on Windows and macOS. Does not use Unix fork. Essentially spawn creates a new process (with no access to parent's resources, so they has to be passed).\n",
    "\n",
    "* **fork** : \n",
    "The parent process uses os.fork() to fork the Python interpreter. The child process, when it begins, is effectively identical to the parent process. All resources of the parent are inherited by the child process. Note that safely forking a multithreaded process is problematic.\n",
    "Available on Unix only. The default on Unix.\n",
    "\n",
    "* **forkserver** : \n",
    "When the program starts and selects the forkserver start method, a server process is started. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded so it is safe for it to use os.fork(). No unnecessary resources are inherited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JVM does not support fork** : \n",
    "\n",
    "* In C and many other programming languages the straightforward way to achieve multiprocessing is to fork separate processes (```fork()``` from C++ with explanation: https://www.csl.mtu.edu/cs4411.ck/www/NOTES/process/fork/create.html).  \n",
    "\n",
    "* Java doesn’t support the concept of a fork (i.e. creating a copy of a running process). \n",
    "\n",
    "* Instead, all you can do is to start up a completely new process. \n",
    "\n",
    "* To create a mirror copy of your current process you’d need to start a new JVM instance with a recreated classpath and make sure that the new process reaches a state where you can get useful results from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using java from Python via JPype:\n",
    "\n",
    "* fork creates a copy of the original process including the previous running JVM. In this case the forked JVM does not work as expected. Older versions of JPype (0.6.x) would allow the forked version to call startJVM which would create a big memory leak. The current version 0.7.1 gives and exception that the JVM cannot be restarted.\n",
    "\n",
    "* When using JPype with multiprocessing it is necessary to spawn rather than to fork. Forked copies inherit a non functional JVM which leads to random issues.\n",
    "\n",
    "* The problem is with the nature of multiprocessing. Python can either fork or spawn a new process. The fork option appears to have significant problems with the JVM. The default on linux is fork.\n",
    "\n",
    "* Using the spawn context (multiprocessing.get_context(\"spawn\")) to create a spawned version of Python will allow a fresh JVM to be created. Each spawned copy is completely independent. There are examples in the subrun.py in the test directory on github as that is what is used to test different JVM options for JPype.\n",
    "\n",
    "* If you are using **threads** (rather than processes), all threads share the same JVM and do not need to the JVM independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Threads and processes:\n",
    "\n",
    "* Both processes and threads are independent sequences of execution. \n",
    "* The typical difference is that threads (of the same process) run in a shared memory space, while processes run in separate memory spaces. \n",
    "* Threads are an operating environment feature, rather than a CPU feature (though the CPU typically has operations that make threads efficient).\n",
    "* more process and thread difference https://www.tutorialspoint.com/difference-between-process-and-thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing functionality within this package requires that the __main__ module be importable by the children. Some examples, such as the multiprocessing.pool.Pool examples will not work in the interactive interpreter. \n",
    "\n",
    "Forked processes will work in jupyter cells, but not spawned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x=2\nx=3x=0\nx=4\n\nx=5\nx=1x=6\nx=7\n\nx=8\nx=9\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "from multiprocessing import Pool, Process, Queue, Pipe, Value, Array\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "def f(x):\n",
    "    print(f'x={x}')\n",
    "    return x*x\n",
    "\n",
    "with Pool(2) as pool:  # create pool of 2 processes\n",
    "        pool.map(f, np.arange(10))\n",
    "\n",
    "# output mess due to multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x=[0 1 2 3 4 5 6 7 8 9]\n"
    }
   ],
   "source": [
    "p = Process(target=f, args=(np.arange(10),))  # Process class is used to spawn a process\n",
    "p.start()  # then start() is called\n",
    "p.join()  # wait for process to finish\n",
    "\n",
    "# why such output???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Hello\nparent process: 9283\nprocess id: 9285\n\n\nWorld\nparent process:9285\nprocess id:11652\n"
    }
   ],
   "source": [
    "# checking process ids in mutliprocessing context (can observe 2 different processes)\n",
    "def info(msg):\n",
    "    print(msg)\n",
    "    print('parent process:', os.getppid())\n",
    "    print('process id:', os.getpid())\n",
    "\n",
    "info('Hello')\n",
    "print('\\n')\n",
    "p = Process(target=info, args=('World',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exchanging objects between processes (Queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[42, None, 'hello']\n"
    }
   ],
   "source": [
    "# using Queue to put and get variables\n",
    "\n",
    "def f(q):\n",
    "    q.put([42, None, 'hello'])\n",
    "\n",
    "q = Queue()\n",
    "p = Process(target=f, args=(q,))\n",
    "p.start()\n",
    "print(q.get())    \n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[42, None, 'hello']\n"
    }
   ],
   "source": [
    "# The Pipe() function returns a pair of connection objects connected by a pipe which by default is duplex (two-way). \n",
    "def f(conn):\n",
    "    conn.send([42, None, 'hello'])  # each connection object has send() and recv() methods\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parent_conn, child_conn = Pipe()\n",
    "    p = Process(target=f, args=(child_conn,))\n",
    "    p.start()\n",
    "    print(parent_conn.recv())   \n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that data in a pipe may become corrupted if two processes (or threads) try to read from or write to the same end of the pipe at the same time. Of course there is no risk of corruption from processes using different ends of the pipe at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3.1415927\n[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]\n"
    }
   ],
   "source": [
    "# Data can be stored in a shared memory map using Value or Array\n",
    "\n",
    "def f(n, a):\n",
    "    n.value = 3.1415927\n",
    "    for i in range(len(a)):\n",
    "        a[i] = -a[i]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num = Value('d', 0.0)  # double\n",
    "    arr = Array('i', range(10))  # int array\n",
    "\n",
    "    p = Process(target=f, args=(num, arr))\n",
    "    p.start()\n",
    "    p.join()\n",
    "    print(num.value)\n",
    "    print(arr[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on Pool methods ```map, apply``` and ```apply_async```:\n",
    "\n",
    "* in older python versions ```apply(f, args, kwargs)``` was used, now it's replaced by ```f(*args, **kwargs)```, so apply() is not really used\n",
    "* Pool is mimicking the same behaviour  \n",
    "* pool.apply applies a function (through 1 process) and waits until it's finished and returns a result\n",
    "* pool.apply_async applies a function but returns AsyncResult, the current process can continue further, the function call output can be later retrieved using .get() which waits for the function to finish\n",
    "* so pool.apply(..) is equivalent to pool.apply_async(..).get()\n",
    "* apply_async has callback argument which is called when the function is complete (see example below)\n",
    "* pool.map :\n",
    "    - equivalent to built-in map but accepts only one iterable (!) \n",
    "    - will wait the functions to finish \n",
    "    - will preserve the passed argument order\n",
    "    - will execute mapping for different arguments from the iterable in parallel\n",
    "\n",
    "* pool.map_async - similar to pool.map but won't block the current parent process until the functions finish\n",
    "* pool.starmap will apply * to the args, so we can pass multiple args to the function from an iterable of arg tuples\n",
    "\n",
    "### Here is a table summary of all possible combinations\n",
    "\n",
    "| method                | Multi-args  | Concurrence  |  Blocking    | Ordered-results|\n",
    "-------------------|------------|-----------------|------------|---------|\n",
    "Pool.map          | no          | yes            |yes         | yes|\n",
    "Pool.map_async    | no          | yes            |no          | yes|\n",
    "Pool.apply        | yes         | no             |yes         | no|\n",
    "Pool.apply_async  | yes         | yes            |no          | no|\n",
    "Pool.starmap      | yes         | yes            |yes         | yes|\n",
    "Pool.starmap_async| yes         | yes            |no          | no|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 1, 4, 9]\n"
    }
   ],
   "source": [
    "# callback example for apply_async\n",
    "\n",
    "def foo_pool(x):\n",
    "    time.sleep(1)\n",
    "    return x*x\n",
    "\n",
    "result_list = []\n",
    "def log_result(result):\n",
    "    # This is called whenever foo_pool(i) returns a result.\n",
    "    # result_list is modified only by the main process, not the pool workers.\n",
    "    result_list.append(result)\n",
    "\n",
    "def apply_async_with_callback():\n",
    "    pool = mp.Pool()\n",
    "    for i in range(4):\n",
    "        pool.apply_async(foo_pool, args = (i, ), callback = log_result)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(result_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    apply_async_with_callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n400\n12140\n[12140, 12140, 12140, 12140]\n"
    }
   ],
   "source": [
    "# some examples\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "\n",
    "with Pool(processes=2) as pool:  # start 2 worker processes\n",
    "\n",
    "    print(pool.map(f, range(10)))\n",
    "\n",
    "    # evaluate \"f(20)\" asynchronously\n",
    "    res = pool.apply_async(f, (20,))      # runs in *only* one process\n",
    "    print(res.get(timeout=1))             # prints \"400\"\n",
    "\n",
    "    # evaluate \"os.getpid()\" asynchronously\n",
    "    res = pool.apply_async(os.getpid, ()) # runs in *only* one process\n",
    "    print(res.get(timeout=1))             # prints the PID of that process\n",
    "\n",
    "    # launching multiple evaluations asynchronously *may* use more processes\n",
    "    multiple_results = [pool.apply_async(os.getpid, ()) for i in range(4)]\n",
    "    print([res.get(timeout=1) for res in multiple_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple ways to iterate over some of the function arguments and fix the other \n",
    "# say we have f(a, b), we want to fix b=10 on the fly and apply f(a, b=10) to a list of a-values\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def func(a, b):\n",
    "    return a + b\n",
    "\n",
    "def main():\n",
    "    a_args = [1,2,3]\n",
    "    second_arg = 1\n",
    "    with Pool() as pool:\n",
    "        L = pool.starmap(func, [(1, 1), (2, 1), (3, 1)])\n",
    "        M = pool.starmap(func, zip(a_args, repeat(second_arg)))\n",
    "        N = pool.map(partial(func, b=second_arg), a_args)\n",
    "        assert L == M == N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using functools.partial is very helpful (!!) since it's output created on the fly can be used both with fork and spawn, while other local functions (e.g. defined inside current function, not globally) cannot be pickled and are not suitable for multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (add multithreading in python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}